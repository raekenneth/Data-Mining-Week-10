{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JuFb1MP_O5EW",
    "outputId": "17a158d8-8df8-4af8-d8cb-b15b95e590c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlxtend in /Users/rae/anaconda3/lib/python3.10/site-packages (0.23.0)\n",
      "Requirement already satisfied: scipy>=1.2.1 in /Users/rae/anaconda3/lib/python3.10/site-packages (from mlxtend) (1.10.0)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in /Users/rae/anaconda3/lib/python3.10/site-packages (from mlxtend) (3.7.0)\n",
      "Requirement already satisfied: joblib>=0.13.2 in /Users/rae/anaconda3/lib/python3.10/site-packages (from mlxtend) (1.1.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /Users/rae/anaconda3/lib/python3.10/site-packages (from mlxtend) (1.2.1)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /Users/rae/anaconda3/lib/python3.10/site-packages (from mlxtend) (1.5.3)\n",
      "Requirement already satisfied: numpy>=1.16.2 in /Users/rae/anaconda3/lib/python3.10/site-packages (from mlxtend) (1.23.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/rae/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.0.0->mlxtend) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/rae/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.0.0->mlxtend) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/rae/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.0.0->mlxtend) (1.0.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/rae/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.0.0->mlxtend) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/rae/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.0.0->mlxtend) (4.25.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/rae/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.0.0->mlxtend) (9.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rae/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.0.0->mlxtend) (22.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/rae/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.0.0->mlxtend) (3.0.9)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/rae/anaconda3/lib/python3.10/site-packages (from pandas>=0.24.2->mlxtend) (2022.7)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/rae/anaconda3/lib/python3.10/site-packages (from scikit-learn>=1.0.2->mlxtend) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/rae/anaconda3/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "! pip install mlxtend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDYkmRlZPSXY"
   },
   "source": [
    "# Association Rule for Store Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYfOAa9fPjln"
   },
   "source": [
    "In this case study, we will explore how association rule can be used to analyze the items that are usualy purcased together.\n",
    "\n",
    "you can refer to this article to find out about apriori and association rule:\n",
    "https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/apriori/\n",
    "https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/association_rules/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EOg6BIYPxt4"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gp0OZCjrQT1n"
   },
   "source": [
    "We will use the dataset of the transaction in a certain store. You can get the dataset here: \n",
    "https://gist.githubusercontent.com/Harsh-Git-Hub/2979ec48043928ad9033d8469928e751/raw/72de943e040b8bd0d087624b154d41b2ba9d9b60/retail_dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "LDF65VBRQjFL",
    "outputId": "e9c6b17c-3450-4b26-af48-e963cc0dd11b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0       1     2       3       4       5       6\n",
      "0   Bread    Wine  Eggs    Meat  Cheese  Pencil  Diaper\n",
      "1   Bread  Cheese  Meat  Diaper    Wine    Milk  Pencil\n",
      "2  Cheese    Meat  Eggs    Milk    Wine     NaN     NaN\n",
      "3  Cheese    Meat  Eggs    Milk    Wine     NaN     NaN\n",
      "4    Meat  Pencil  Wine     NaN     NaN     NaN     NaN\n"
     ]
    }
   ],
   "source": [
    "# load the data set and show the first five transaction\n",
    "\n",
    "url = \"https://gist.githubusercontent.com/Harsh-Git-Hub/2979ec48043928ad9033d8469928e751/raw/72de943e040b8bd0d087624b154d41b2ba9d9b60/retail_dataset.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkfhUabDQup9"
   },
   "source": [
    "# Get the set of product that has been purchased\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the unique product that has been purchased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Awz6VzuMwR_-",
    "outputId": "1fc181b3-cffe-48fe-9d3b-066323061907"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Diaper', 'Bagel', 'Meat', 'Milk', 'Wine', 'Eggs', 'Pencil', nan, 'Cheese', 'Bread'}\n"
     ]
    }
   ],
   "source": [
    "unique_products = set(data['6'].unique())\n",
    "\n",
    "print(unique_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4g4k83bP07H"
   },
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEnL1bXtRLXe"
   },
   "source": [
    "In this step, we will transform our dataset so that we will have a one hot encoding based on the purchased products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "N4wdVmFWQ_yg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Bagel  Bread  Cheese  Diaper  Eggs  Meat  Milk  Pencil  Wine\n",
      "5                                                                   \n",
      "Bagel       0      0       0       0     0     1     1       1     1\n",
      "Bread       0      0       0       0     1     0     0       0     0\n",
      "Cheese      0      1       0       0     0     0     1       0     1\n",
      "Diaper      0      0       0       0     1     1     0       1     1\n",
      "Eggs        0      0       1       1     0     1     1       0     0\n",
      "Meat        1      0       0       0     1     0     0       1     1\n",
      "Milk        1      1       0       1     0     1     0       1     0\n",
      "Pencil      0      0       1       0     1     1     0       0     1\n",
      "Wine        0      0       0       0     0     1     0       0     0\n",
      "\n",
      "\n",
      "        0       1     2       3       5       6  4_Bagel  4_Bread  4_Cheese  \\\n",
      "0   Bread    Wine  Eggs    Meat  Pencil  Diaper        0        0         1   \n",
      "1   Bread  Cheese  Meat  Diaper    Milk  Pencil        0        0         0   \n",
      "2  Cheese    Meat  Eggs    Milk     NaN     NaN        0        0         0   \n",
      "3  Cheese    Meat  Eggs    Milk     NaN     NaN        0        0         0   \n",
      "4    Meat  Pencil  Wine     NaN     NaN     NaN        0        0         0   \n",
      "\n",
      "   4_Diaper  4_Eggs  4_Meat  4_Milk  4_Pencil  4_Wine  \n",
      "0         0       0       0       0         0       0  \n",
      "1         0       0       0       0         0       1  \n",
      "2         0       0       0       0         0       1  \n",
      "3         0       0       0       0         0       1  \n",
      "4         0       0       0       0         0       0  \n"
     ]
    }
   ],
   "source": [
    "#create an itemset based on the products\n",
    "\n",
    "transactions = data.groupby(['1'])['1'].apply(list)\n",
    "\n",
    "itemset = pd.get_dummies(data['2']).groupby(data['5']).max()\n",
    "\n",
    "print(itemset)\n",
    "print()\n",
    "print()\n",
    "\n",
    "\n",
    "# encoding the feature\n",
    "\n",
    "encoded_data = pd.get_dummies(data, columns=['4'])\n",
    "\n",
    "print(encoded_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "v67eBdxByEJX",
    "outputId": "b05c05fb-7ae1-4fbe-a01a-d6985c360f5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0_Bagel  0_Bread  0_Cheese  0_Diaper  0_Eggs  0_Meat  0_Milk  0_Pencil  \\\n",
      "0        0        1         0         0       0       0       0         0   \n",
      "1        0        1         0         0       0       0       0         0   \n",
      "2        0        0         1         0       0       0       0         0   \n",
      "3        0        0         1         0       0       0       0         0   \n",
      "4        0        0         0         0       0       1       0         0   \n",
      "\n",
      "   0_Wine  1_Bagel  ...  5_Wine  6_Bagel  6_Bread  6_Cheese  6_Diaper  6_Eggs  \\\n",
      "0       0        0  ...       0        0        0         0         1       0   \n",
      "1       0        0  ...       0        0        0         0         0       0   \n",
      "2       0        0  ...       0        0        0         0         0       0   \n",
      "3       0        0  ...       0        0        0         0         0       0   \n",
      "4       0        0  ...       0        0        0         0         0       0   \n",
      "\n",
      "   6_Meat  6_Milk  6_Pencil  6_Wine  \n",
      "0       0       0         0       0  \n",
      "1       0       0         1       0  \n",
      "2       0       0         0       0  \n",
      "3       0       0         0       0  \n",
      "4       0       0         0       0  \n",
      "\n",
      "[5 rows x 63 columns]\n"
     ]
    }
   ],
   "source": [
    "  # create new dataframe from the encoded features\n",
    "\n",
    "  # show the new dataframe\n",
    "\n",
    "encoded_data = pd.get_dummies(data, columns=['0','1','2','3','4','5','6'])\n",
    "\n",
    "new_dataframe = encoded_data.drop([], axis=1)\n",
    "\n",
    "print(new_dataframe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBJmzWAAS4Mw"
   },
   "source": [
    "Since, the encoded dataframe consist of the empty column. We will drop the NaN column or select all columns other than the first column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "2eHZu15xyTqm",
    "outputId": "7bffff16-fc02-48fe-bc98-7616bf75908e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0_Bagel  0_Bread  0_Cheese  0_Diaper  0_Eggs  0_Meat  0_Milk  0_Pencil  \\\n",
      "0        0        1         0         0       0       0       0         0   \n",
      "1        0        1         0         0       0       0       0         0   \n",
      "2        0        0         1         0       0       0       0         0   \n",
      "3        0        0         1         0       0       0       0         0   \n",
      "4        0        0         0         0       0       1       0         0   \n",
      "\n",
      "   0_Wine  1_Bagel  ...  5_Wine  6_Bagel  6_Bread  6_Cheese  6_Diaper  6_Eggs  \\\n",
      "0       0        0  ...       0        0        0         0         1       0   \n",
      "1       0        0  ...       0        0        0         0         0       0   \n",
      "2       0        0  ...       0        0        0         0         0       0   \n",
      "3       0        0  ...       0        0        0         0         0       0   \n",
      "4       0        0  ...       0        0        0         0         0       0   \n",
      "\n",
      "   6_Meat  6_Milk  6_Pencil  6_Wine  \n",
      "0       0       0         0       0  \n",
      "1       0       0         1       0  \n",
      "2       0       0         0       0  \n",
      "3       0       0         0       0  \n",
      "4       0       0         0       0  \n",
      "\n",
      "[5 rows x 63 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encoded_data = pd.get_dummies(data, columns=['0','1','2','3','4','5','6'])\n",
    "\n",
    "encoded_data.dropna(axis=1, how='any', inplace=True)\n",
    "\n",
    "print(encoded_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0_Bread  0_Cheese  0_Diaper  0_Eggs  0_Meat  0_Milk  0_Pencil  0_Wine  \\\n",
      "0        1         0         0       0       0       0         0       0   \n",
      "1        1         0         0       0       0       0         0       0   \n",
      "2        0         1         0       0       0       0         0       0   \n",
      "3        0         1         0       0       0       0         0       0   \n",
      "4        0         0         0       0       1       0         0       0   \n",
      "\n",
      "   1_Bagel  1_Bread  ...  5_Wine  6_Bagel  6_Bread  6_Cheese  6_Diaper  \\\n",
      "0        0        0  ...       0        0        0         0         1   \n",
      "1        0        0  ...       0        0        0         0         0   \n",
      "2        0        0  ...       0        0        0         0         0   \n",
      "3        0        0  ...       0        0        0         0         0   \n",
      "4        0        0  ...       0        0        0         0         0   \n",
      "\n",
      "   6_Eggs  6_Meat  6_Milk  6_Pencil  6_Wine  \n",
      "0       0       0       0         0       0  \n",
      "1       0       0       0         1       0  \n",
      "2       0       0       0         0       0  \n",
      "3       0       0       0         0       0  \n",
      "4       0       0       0         0       0  \n",
      "\n",
      "[5 rows x 62 columns]\n"
     ]
    }
   ],
   "source": [
    "encoded_data = pd.get_dummies(data, columns=['0','1','2','3','4','5','6'])\n",
    "\n",
    "selected_columns = encoded_data.iloc[:, 1:]\n",
    "\n",
    "print(selected_columns.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UXDzSNPP35P"
   },
   "source": [
    "## Apriori Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-jD3ea4TYMV"
   },
   "source": [
    "We will use appriori algorithm to determine the frequently purchased products. \n",
    "For this case study, we will min_support=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     support                            itemsets\n",
      "1   0.234921                           (0_Bread)\n",
      "2   0.177778                          (0_Cheese)\n",
      "21  0.165079                            (2_Eggs)\n",
      "13  0.149206                            (1_Meat)\n",
      "8   0.149206                           (1_Bagel)\n",
      "30  0.142857                            (3_Milk)\n",
      "23  0.142857                            (2_Milk)\n",
      "34  0.114286                            (4_Wine)\n",
      "10  0.111111                          (1_Cheese)\n",
      "45  0.111111                    (3_Milk, 2_Eggs)\n",
      "3   0.107937                          (0_Diaper)\n",
      "43  0.104762                    (1_Meat, 3_Milk)\n",
      "36  0.104762                   (2_Milk, 0_Bread)\n",
      "50  0.101587          (0_Cheese, 3_Milk, 2_Eggs)\n",
      "39  0.101587                  (0_Cheese, 3_Milk)\n",
      "38  0.101587                  (0_Cheese, 2_Eggs)\n",
      "5   0.101587                            (0_Meat)\n",
      "48  0.098413          (0_Cheese, 1_Meat, 2_Eggs)\n",
      "49  0.098413          (0_Cheese, 1_Meat, 3_Milk)\n",
      "42  0.098413                    (1_Meat, 2_Eggs)\n",
      "37  0.098413                  (0_Cheese, 1_Meat)\n",
      "51  0.098413            (1_Meat, 3_Milk, 2_Eggs)\n",
      "0   0.098413                           (0_Bagel)\n",
      "52  0.098413  (0_Cheese, 1_Meat, 3_Milk, 2_Eggs)\n",
      "11  0.095238                          (1_Diaper)\n",
      "35  0.095238                  (0_Bread, 1_Bagel)\n",
      "16  0.085714                            (1_Wine)\n",
      "41  0.085714                   (2_Milk, 1_Bagel)\n",
      "9   0.085714                           (1_Bread)\n",
      "47  0.085714          (2_Milk, 0_Bread, 1_Bagel)\n",
      "14  0.085714                            (1_Milk)\n",
      "25  0.085714                            (2_Wine)\n",
      "27  0.082540                          (3_Cheese)\n",
      "6   0.082540                          (0_Pencil)\n",
      "22  0.079365                            (2_Meat)\n",
      "4   0.076190                            (0_Eggs)\n",
      "7   0.073016                            (0_Wine)\n",
      "12  0.073016                            (1_Eggs)\n",
      "15  0.069841                          (1_Pencil)\n",
      "18  0.066667                           (2_Bread)\n",
      "26  0.063492                           (3_Bagel)\n",
      "20  0.063492                          (2_Diaper)\n",
      "24  0.060317                          (2_Pencil)\n",
      "40  0.057143                  (0_Cheese, 4_Wine)\n",
      "17  0.057143                           (2_Bagel)\n",
      "19  0.057143                          (2_Cheese)\n",
      "44  0.053968                    (1_Meat, 4_Wine)\n",
      "31  0.053968                          (3_Pencil)\n",
      "29  0.053968                            (3_Meat)\n",
      "28  0.053968                            (3_Eggs)\n",
      "33  0.050794                            (4_Meat)\n",
      "46  0.050794                    (4_Wine, 3_Milk)\n",
      "32  0.050794                            (3_Wine)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rae/anaconda3/lib/python3.10/site-packages/mlxtend/frequent_patterns/fpcommon.py:110: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoded_data = pd.get_dummies(data, columns=['0','1','2','3','4','5','6'])\n",
    "\n",
    "frequent_itemsets = apriori(encoded_data, min_support=0.05, use_colnames=True)\n",
    "\n",
    "frequent_itemsets = frequent_itemsets.sort_values(by='support', ascending=False)\n",
    "\n",
    "with pd.option_context('display.max_rows', None, 'display.max_colwidth', None):\n",
    "    print(frequent_itemsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEr2YXHrVOIA"
   },
   "source": [
    "Then, we will generate association rule of the frequent itemset based on confidence level with the threshold=0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "id": "5GalSXOoy6H8",
    "outputId": "2fc5a421-bca1-41c8-f96a-e24f49280d99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   antecedents                 consequents  \\\n",
      "0                    (1_Bagel)                   (0_Bread)   \n",
      "1                     (2_Milk)                   (0_Bread)   \n",
      "2                     (1_Meat)                  (0_Cheese)   \n",
      "3                     (2_Eggs)                  (0_Cheese)   \n",
      "4                     (3_Milk)                  (0_Cheese)   \n",
      "5                     (2_Milk)                   (1_Bagel)   \n",
      "6                     (1_Meat)                    (2_Eggs)   \n",
      "7                     (1_Meat)                    (3_Milk)   \n",
      "8                     (3_Milk)                    (1_Meat)   \n",
      "9                     (3_Milk)                    (2_Eggs)   \n",
      "10                    (2_Eggs)                    (3_Milk)   \n",
      "11           (2_Milk, 0_Bread)                   (1_Bagel)   \n",
      "12           (2_Milk, 1_Bagel)                   (0_Bread)   \n",
      "13          (0_Bread, 1_Bagel)                    (2_Milk)   \n",
      "14                    (2_Milk)          (0_Bread, 1_Bagel)   \n",
      "15          (0_Cheese, 1_Meat)                    (2_Eggs)   \n",
      "16          (0_Cheese, 2_Eggs)                    (1_Meat)   \n",
      "17            (1_Meat, 2_Eggs)                  (0_Cheese)   \n",
      "18                    (1_Meat)          (0_Cheese, 2_Eggs)   \n",
      "19          (0_Cheese, 1_Meat)                    (3_Milk)   \n",
      "20          (0_Cheese, 3_Milk)                    (1_Meat)   \n",
      "21            (1_Meat, 3_Milk)                  (0_Cheese)   \n",
      "22                    (1_Meat)          (0_Cheese, 3_Milk)   \n",
      "23                    (3_Milk)          (0_Cheese, 1_Meat)   \n",
      "24          (0_Cheese, 3_Milk)                    (2_Eggs)   \n",
      "25          (0_Cheese, 2_Eggs)                    (3_Milk)   \n",
      "26            (3_Milk, 2_Eggs)                  (0_Cheese)   \n",
      "27                    (3_Milk)          (0_Cheese, 2_Eggs)   \n",
      "28                    (2_Eggs)          (0_Cheese, 3_Milk)   \n",
      "29            (1_Meat, 3_Milk)                    (2_Eggs)   \n",
      "30            (1_Meat, 2_Eggs)                    (3_Milk)   \n",
      "31            (3_Milk, 2_Eggs)                    (1_Meat)   \n",
      "32                    (1_Meat)            (3_Milk, 2_Eggs)   \n",
      "33                    (3_Milk)            (1_Meat, 2_Eggs)   \n",
      "34  (0_Cheese, 1_Meat, 3_Milk)                    (2_Eggs)   \n",
      "35  (0_Cheese, 1_Meat, 2_Eggs)                    (3_Milk)   \n",
      "36  (0_Cheese, 3_Milk, 2_Eggs)                    (1_Meat)   \n",
      "37    (1_Meat, 3_Milk, 2_Eggs)                  (0_Cheese)   \n",
      "38          (0_Cheese, 1_Meat)            (3_Milk, 2_Eggs)   \n",
      "39          (0_Cheese, 3_Milk)            (1_Meat, 2_Eggs)   \n",
      "40          (0_Cheese, 2_Eggs)            (1_Meat, 3_Milk)   \n",
      "41            (1_Meat, 3_Milk)          (0_Cheese, 2_Eggs)   \n",
      "42            (1_Meat, 2_Eggs)          (0_Cheese, 3_Milk)   \n",
      "43            (3_Milk, 2_Eggs)          (0_Cheese, 1_Meat)   \n",
      "44                    (1_Meat)  (0_Cheese, 3_Milk, 2_Eggs)   \n",
      "45                    (3_Milk)  (0_Cheese, 1_Meat, 2_Eggs)   \n",
      "\n",
      "    antecedent support  consequent support   support  confidence      lift  \\\n",
      "0             0.149206            0.234921  0.095238    0.638298  2.717079   \n",
      "1             0.142857            0.234921  0.104762    0.733333  3.121622   \n",
      "2             0.149206            0.177778  0.098413    0.659574  3.710106   \n",
      "3             0.165079            0.177778  0.101587    0.615385  3.461538   \n",
      "4             0.142857            0.177778  0.101587    0.711111  4.000000   \n",
      "5             0.142857            0.149206  0.085714    0.600000  4.021277   \n",
      "6             0.149206            0.165079  0.098413    0.659574  3.995499   \n",
      "7             0.149206            0.142857  0.104762    0.702128  4.914894   \n",
      "8             0.142857            0.149206  0.104762    0.733333  4.914894   \n",
      "9             0.142857            0.165079  0.111111    0.777778  4.711538   \n",
      "10            0.165079            0.142857  0.111111    0.673077  4.711538   \n",
      "11            0.104762            0.149206  0.085714    0.818182  5.483559   \n",
      "12            0.085714            0.234921  0.085714    1.000000  4.256757   \n",
      "13            0.095238            0.142857  0.085714    0.900000  6.300000   \n",
      "14            0.142857            0.095238  0.085714    0.600000  6.300000   \n",
      "15            0.098413            0.165079  0.098413    1.000000  6.057692   \n",
      "16            0.101587            0.149206  0.098413    0.968750  6.492686   \n",
      "17            0.098413            0.177778  0.098413    1.000000  5.625000   \n",
      "18            0.149206            0.101587  0.098413    0.659574  6.492686   \n",
      "19            0.098413            0.142857  0.098413    1.000000  7.000000   \n",
      "20            0.101587            0.149206  0.098413    0.968750  6.492686   \n",
      "21            0.104762            0.177778  0.098413    0.939394  5.284091   \n",
      "22            0.149206            0.101587  0.098413    0.659574  6.492686   \n",
      "23            0.142857            0.098413  0.098413    0.688889  7.000000   \n",
      "24            0.101587            0.165079  0.101587    1.000000  6.057692   \n",
      "25            0.101587            0.142857  0.101587    1.000000  7.000000   \n",
      "26            0.111111            0.177778  0.101587    0.914286  5.142857   \n",
      "27            0.142857            0.101587  0.101587    0.711111  7.000000   \n",
      "28            0.165079            0.101587  0.101587    0.615385  6.057692   \n",
      "29            0.104762            0.165079  0.098413    0.939394  5.690559   \n",
      "30            0.098413            0.142857  0.098413    1.000000  7.000000   \n",
      "31            0.111111            0.149206  0.098413    0.885714  5.936170   \n",
      "32            0.149206            0.111111  0.098413    0.659574  5.936170   \n",
      "33            0.142857            0.098413  0.098413    0.688889  7.000000   \n",
      "34            0.098413            0.165079  0.098413    1.000000  6.057692   \n",
      "35            0.098413            0.142857  0.098413    1.000000  7.000000   \n",
      "36            0.101587            0.149206  0.098413    0.968750  6.492686   \n",
      "37            0.098413            0.177778  0.098413    1.000000  5.625000   \n",
      "38            0.098413            0.111111  0.098413    1.000000  9.000000   \n",
      "39            0.101587            0.098413  0.098413    0.968750  9.843750   \n",
      "40            0.101587            0.104762  0.098413    0.968750  9.247159   \n",
      "41            0.104762            0.101587  0.098413    0.939394  9.247159   \n",
      "42            0.098413            0.101587  0.098413    1.000000  9.843750   \n",
      "43            0.111111            0.098413  0.098413    0.885714  9.000000   \n",
      "44            0.149206            0.101587  0.098413    0.659574  6.492686   \n",
      "45            0.142857            0.098413  0.098413    0.688889  7.000000   \n",
      "\n",
      "    leverage  conviction  zhangs_metric  \n",
      "0   0.060186    2.115219       0.742786  \n",
      "1   0.071202    2.869048       0.792929  \n",
      "2   0.071887    2.415278       0.858570  \n",
      "3   0.072240    2.137778       0.851711  \n",
      "4   0.076190    2.846154       0.875000  \n",
      "5   0.064399    2.126984       0.876543  \n",
      "6   0.073782    2.452579       0.881199  \n",
      "7   0.083447    2.877551       0.936228  \n",
      "8   0.083447    3.190476       0.929293  \n",
      "9   0.087528    3.757143       0.919048  \n",
      "10  0.087528    2.621849       0.943509  \n",
      "11  0.070083    4.679365       0.913318  \n",
      "12  0.065578         inf       0.836806  \n",
      "13  0.072109    8.571429       0.929825  \n",
      "14  0.072109    2.261905       0.981481  \n",
      "15  0.082167         inf       0.926056  \n",
      "16  0.083255   27.225397       0.941639  \n",
      "17  0.080917         inf       0.911972  \n",
      "18  0.083255    2.639087       0.994343  \n",
      "19  0.084354         inf       0.950704  \n",
      "20  0.083255   27.225397       0.941639  \n",
      "21  0.079788   13.566667       0.905628  \n",
      "22  0.083255    2.639087       0.994343  \n",
      "23  0.084354    2.897959       1.000000  \n",
      "24  0.084817         inf       0.929329  \n",
      "25  0.087075         inf       0.954064  \n",
      "26  0.081834    9.592593       0.906250  \n",
      "27  0.087075    3.109890       1.000000  \n",
      "28  0.084817    2.335873       1.000000  \n",
      "29  0.081119   13.776190       0.920728  \n",
      "30  0.084354         inf       0.950704  \n",
      "31  0.081834    7.444444       0.935484  \n",
      "32  0.081834    2.611111       0.977371  \n",
      "33  0.084354    2.897959       1.000000  \n",
      "34  0.082167         inf       0.926056  \n",
      "35  0.084354         inf       0.950704  \n",
      "36  0.083255   27.225397       0.941639  \n",
      "37  0.080917         inf       0.911972  \n",
      "38  0.087478         inf       0.985915  \n",
      "39  0.088415   28.850794       1.000000  \n",
      "40  0.087770   28.647619       0.992705  \n",
      "41  0.087770   14.823810       0.996225  \n",
      "42  0.088415         inf       0.996479  \n",
      "43  0.087478    7.888889       1.000000  \n",
      "44  0.083255    2.639087       0.994343  \n",
      "45  0.084354    2.897959       1.000000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rae/anaconda3/lib/python3.10/site-packages/mlxtend/frequent_patterns/fpcommon.py:110: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoded_data = pd.get_dummies(data, columns=['0','1','2','3','4','5','6'])\n",
    "\n",
    "frequent_itemsets = apriori(encoded_data, min_support=0.05, use_colnames=True)\n",
    "\n",
    "association_rules_output = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.6)\n",
    "\n",
    "print(association_rules_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide explanation about __antecedent support__, __consequent support__, __support__, __confidence__, __lift__, __leverage__ and __conviction__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Antecedent Support: The support of the antecedent part of the rule.\n",
    "\n",
    "#Consequent Support: The support of the consequent part of the rule.\n",
    "\n",
    "#Support: The support of an itemset measures how frequently the itemset appears in the dataset. Higher support indicates a more frequently occurring itemset.\n",
    "\n",
    "#Confidence: measures how often a rule is found to be true. Higher confidence signifies that when {A} is present, the likelihood of {B} being present as well.\n",
    "\n",
    "#Lift: measures how much more often the antecedent (if) and consequent (then) of a rule occur together than we would expect if they were statistically independent.\n",
    "\n",
    "#Leverage: measures the difference between the observed frequency of {A, B} appearing together and what would be expected if A and B were independent.\n",
    "\n",
    "#Conviction: measures the ratio of the expected frequency that A occurs without B (if they were independent) to the observed frequency of incorrect predictions. High conviction means that the consequent is highly dependent on the antecedent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Markdown and Latex: aÂº"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
